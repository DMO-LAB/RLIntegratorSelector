{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import gymnasium as gym\n",
    "from environment import SimulationSettings, create_env\n",
    "from rl_trainer import Agent, Args\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = 'run/rl_train/env_benchmark.h5'\n",
    "\n",
    "data = h5py.File(file_path, 'r')\n",
    "data = data['data']\n",
    "temperatures = data['temperatures'][:]\n",
    "phis = data['phis'][:]\n",
    "cpu_times = data['cpu_times'][:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temperatures[3000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_integrator_heuristic(solver) -> List[str]:\n",
    "    \"\"\"Set integrator type based on temperature and equivalence ratio\"\"\"\n",
    "    \n",
    "    # Start with temperature-based decision\n",
    "    integ = np.where(solver.T <= 600.0, 'boostRK', 'cvode')\n",
    "    \n",
    "    try:\n",
    "        # Get equivalence ratio\n",
    "        phi = solver.phi\n",
    "        \n",
    "        # Use boostRK for extreme conditions\n",
    "        integ = np.where(phi == -1, 'boostRK', integ)  # invalid phi\n",
    "        integ = np.where(phi <= 1e-8, 'boostRK', integ)  # oxidizer-dominated\n",
    "        integ = np.where(phi >= 1e4, 'boostRK', integ)   # fuel-dominated\n",
    "        \n",
    "        # Create boolean mask for CVODE points\n",
    "        cvode_mask = (integ == 'cvode')\n",
    "        \n",
    "        # Include neighboring points\n",
    "        cvode_mask_left = np.roll(cvode_mask, 1)\n",
    "        cvode_mask_right = np.roll(cvode_mask, -1)\n",
    "        cvode_mask_left[0] = False\n",
    "        cvode_mask_right[-1] = False\n",
    "        \n",
    "        use_cvode = cvode_mask | cvode_mask_left | cvode_mask_right\n",
    "        integ = np.where(use_cvode, 'cvode', 'boostRK')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate phi for integrator selection: {e}\")\n",
    "    \n",
    "    return integ.tolist()\n",
    "\n",
    "\n",
    "def integ_to_action(integ):\n",
    "    integ = np.array(integ)\n",
    "    action = np.zeros(len(integ))\n",
    "    action[integ == 'boostRK'] = 1\n",
    "    # convert to int\n",
    "    action = action.astype(int)\n",
    "    return action.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_config = {\n",
    "            'local_features': True,\n",
    "            'neighbor_features': False,\n",
    "            'gradient_features': False,\n",
    "            'temporal_features': False,\n",
    "            'window_size': 1\n",
    "        }\n",
    "    \n",
    "\n",
    "reward_config = {\n",
    "    'weights': {\n",
    "        'accuracy': 0.4,\n",
    "        'efficiency': 0.3,\n",
    "        'stability': 0.3\n",
    "            },\n",
    "            'thresholds': {\n",
    "                'time': 0.01,\n",
    "                'error': 1e-3\n",
    "            },\n",
    "            'scaling': {\n",
    "                'time': 0.1,\n",
    "                'error': 1.0\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Environment setup\n",
    "sim_settings = SimulationSettings(\n",
    "    output_dir='run/diffusion_benchmark',\n",
    "    t_end=0.06,\n",
    "    n_points=100,\n",
    "    global_timestep=1e-5\n",
    ")\n",
    "\n",
    "env = create_env(\n",
    "    sim_settings=sim_settings,\n",
    "    benchmark_file=f\"{sim_settings.output_dir}/env_benchmark.h5\",\n",
    "    species_to_track=['CH4', 'O2', 'CO2', 'H2O'],\n",
    "    features_config=features_config,\n",
    "    reward_config=reward_config\n",
    ")\n",
    "\n",
    "# Agent and optimizer\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = \"models/combustion_ppo_1d__1__1736867708/checkpoint_90.pt\"\n",
    "weights = torch.load(trained_model_path)\n",
    "agent.load_state_dict(weights['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "obs = torch.tensor(obs)\n",
    "\n",
    "action, log_prob, entropy, value = agent.get_action_and_value(obs, deterministic=True)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action)\n",
    "plt.plot(obs[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_simulation(env):\n",
    "    obs, info = env.reset()\n",
    "    cpu_times = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not done and not truncated:\n",
    "        # action = set_integrator_heuristic(env.solver)\n",
    "        # action = integ_to_action(action)\n",
    "        action = [1] * len(env.solver.T)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cpu_times.append(info['cpu_time'])\n",
    "        rewards.append(reward)\n",
    "    return cpu_times, rewards\n",
    "\n",
    "cpu_times, rewards = run_full_simulation(env)\n",
    "plt.plot(cpu_times)\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "boostRK_rewards = np.array(rewards)\n",
    "boostRK_cpu_times = np.array(cpu_times)\n",
    "\n",
    "# save to file\n",
    "np.savez(f\"{sim_settings.output_dir}/boostRK_rewards.npz\", rewards=boostRK_rewards, cpu_times=boostRK_cpu_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvode_rewards = np.array(rewards)\n",
    "cvode_cpu_times = np.array(cpu_times)\n",
    "\n",
    "# save to file\n",
    "np.savez(f\"{sim_settings.output_dir}/cvode_rewards.npz\", rewards=cvode_rewards, cpu_times=cvode_cpu_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics_rewards = np.array(rewards)\n",
    "heuristics_cpu_times = np.array(cpu_times)\n",
    "\n",
    "# save to file\n",
    "np.savez(f\"{sim_settings.output_dir}/heuristics_rewards.npz\", rewards=heuristics_rewards, cpu_times=heuristics_cpu_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_compare = 0, 500, 1000, 1500, 2000, 3000\n",
    "\n",
    "fig, ax = plt.subplots(len(points_to_compare)//2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, point in enumerate(points_to_compare):\n",
    "    ax[i//2, i%2].plot(boostRK_rewards[point], label='boostRK')\n",
    "    ax[i//2, i%2].plot(cvode_rewards[point], label='cvode')\n",
    "    ax[i//2, i%2].plot(heuristics_rewards[point], label='heuristics')\n",
    "    ax[i//2, i%2].set_title(f'Rewards at point {point}')\n",
    "    ax[i//2, i%2].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "plt.plot(cvode_cpu_times, label='cvode')\n",
    "plt.plot(heuristics_cpu_times, label='heuristics')\n",
    "plt.plot(boostRK_cpu_times, label='boostRK')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_cvode_cpu_times = np.cumsum(cvode_cpu_times)\n",
    "cummulative_heuristics_cpu_times = np.cumsum(heuristics_cpu_times)\n",
    "cummulative_boostRK_cpu_times = np.cumsum(boostRK_cpu_times)\n",
    "\n",
    "print(f\"cummulative cvode cpu time: {np.sum(cvode_cpu_times)}\")\n",
    "print(f\"cummulative heuristics cpu time: {np.sum(heuristics_cpu_times)}\")\n",
    "print(f\"cummulative boostRK cpu time: {np.sum(boostRK_cpu_times)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_cvode_rewards = np.cumsum(cvode_rewards)\n",
    "cummulative_heuristics_rewards = np.cumsum(heuristics_rewards)\n",
    "cummulative_boostRK_rewards = np.cumsum(boostRK_rewards)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "ax[0].plot(cummulative_cvode_rewards, label='cvode')\n",
    "ax[0].plot(cummulative_heuristics_rewards, label='heuristics')\n",
    "ax[0].plot(cummulative_boostRK_rewards, label='boostRK')\n",
    "ax[0].set_title('Cummulative Rewards')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(cummulative_cvode_cpu_times, label='cvode')\n",
    "ax[1].plot(cummulative_heuristics_cpu_times, label='heuristics')\n",
    "ax[1].plot(cummulative_boostRK_cpu_times, label='boostRK')\n",
    "ax[1].set_title('Cummulative CPU Times')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cvode_cpu_time = np.sum(cvode_cpu_times)\n",
    "total_heuristics_cpu_time = np.sum(heuristics_cpu_times)\n",
    "total_boostRK_cpu_time = np.sum(boostRK_cpu_times)\n",
    "\n",
    "total_cvode_reward = np.sum(cvode_rewards, axis=0)\n",
    "total_heuristics_reward = np.sum(heuristics_rewards, axis=0)\n",
    "total_boostRK_reward = np.sum(boostRK_rewards, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.plot(total_cvode_reward, label='cvode')\n",
    "plt.plot(total_heuristics_reward, label='heuristics')\n",
    "plt.plot(total_boostRK_reward, label='boostRK')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cvode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "ax[0].plot(cvode_rewards[-4000], label='cvode')\n",
    "ax[0].plot(heuristics_rewards[-4000], label='heuristics')\n",
    "ax[1].plot(cvode_cpu_times, label='cvode')\n",
    "ax[1].plot(heuristics_cpu_times, label='heuristics')\n",
    "\n",
    "ax[0].set_title('Rewards')\n",
    "ax[1].set_title('CPU Times')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "import tyro\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import SimulationSettings, create_env\n",
    "from ppo import PPO, RolloutBuffer\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"combustion_ppo_1d\"\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = False\n",
    "    track: bool = False\n",
    "    wandb_project_name: str = \"combustion_control_1d\"\n",
    "    wandb_entity: str = None\n",
    "    \n",
    "    # Environment Parameters\n",
    "    output_dir: str = 'run/rl_test'\n",
    "    t_end: float = 0.06\n",
    "    n_points: int = 100\n",
    "    global_timestep: float = 1e-5\n",
    "    \n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = 100000\n",
    "    learning_rate: float = 2.5e-4\n",
    "    num_envs: int = 1\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    value_loss_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    \n",
    "    # Features configuration\n",
    "    features_config: dict = None\n",
    "    reward_config: dict = None\n",
    "\n",
    "def make_env(args):\n",
    "    \"\"\"Create environment with specified settings\"\"\"\n",
    "    sim_settings = SimulationSettings(\n",
    "        output_dir=args.output_dir,\n",
    "        t_end=args.t_end,\n",
    "        n_points=args.n_points,\n",
    "        global_timestep=args.global_timestep\n",
    "    )\n",
    "    \n",
    "    env = create_env(\n",
    "        sim_settings=sim_settings,\n",
    "        benchmark_file=f\"{args.output_dir}/env_benchmark.h5\",\n",
    "        species_to_track=['CH4', 'O2', 'CO2', 'H2O'],\n",
    "        features_config=args.features_config,\n",
    "        reward_config=args.reward_config\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def log_episode_info(writer, global_step, info):\n",
    "    \"\"\"Log episode information to tensorboard and wandb\"\"\"\n",
    "    log_dict = {}\n",
    "    \n",
    "    if 'point_errors' in info:\n",
    "        mean_error = np.mean(info['point_errors'])\n",
    "        max_error = np.max(info['point_errors'])\n",
    "        log_dict.update({\n",
    "            \"metrics/mean_error\": mean_error,\n",
    "            \"metrics/max_error\": max_error\n",
    "        })\n",
    "    \n",
    "    if 'point_rewards' in info:\n",
    "        mean_reward = np.mean(info['point_rewards'])\n",
    "        min_reward = np.min(info['point_rewards'])\n",
    "        log_dict.update({\n",
    "            \"metrics/mean_reward\": mean_reward,\n",
    "            \"metrics/min_reward\": min_reward\n",
    "        })\n",
    "    \n",
    "    if 'cpu_time' in info:\n",
    "        log_dict[\"metrics/step_time\"] = info['cpu_time']\n",
    "    \n",
    "    if 'total_time' in info:\n",
    "        log_dict[\"metrics/total_time\"] = info['total_time']\n",
    "    \n",
    "    # Log to both tensorboard and wandb\n",
    "    for key, value in log_dict.items():\n",
    "        writer.add_scalar(key, value, global_step)\n",
    "    if wandb.run is not None:\n",
    "        wandb.log(log_dict, step=global_step)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    # Seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    \n",
    "    # Setup run\n",
    "    run_name = f\"{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    \n",
    "    # Environment setup\n",
    "    env = make_env(args)\n",
    "    \n",
    "    # PPO agent setup\n",
    "    state_dim = env.observation_space.shape[1]\n",
    "    action_dim = len(env.integrator_options)\n",
    "    \n",
    "    ppo_agent = PPO(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr_actor=args.learning_rate,\n",
    "        lr_critic=args.learning_rate,\n",
    "        gamma=args.gamma,\n",
    "        K_epochs=args.update_epochs,\n",
    "        eps_clip=args.eps_clip,\n",
    "        has_continuous_action_space=False\n",
    "    )\n",
    "    \n",
    "    # Initialize environment\n",
    "    obs, _ = env.reset(seed=args.seed)\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    # Calculate number of updates\n",
    "    total_timesteps = args.total_timesteps\n",
    "    num_updates = total_timesteps // args.num_steps\n",
    "    \n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Collect trajectory\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1\n",
    "            \n",
    "            # Get actions for all points\n",
    "            actions = []\n",
    "            for point_obs in obs:\n",
    "                action = ppo_agent.select_action(point_obs)\n",
    "                actions.append(action)\n",
    "            actions = np.array(actions)\n",
    "            \n",
    "            # Execute in environment\n",
    "            next_obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "            \n",
    "            # Store step data\n",
    "            for point_idx in range(args.n_points):\n",
    "                # Convert to done flag\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                ppo_agent.buffer.rewards.append(rewards[point_idx])\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "            \n",
    "            # Log episode info\n",
    "            if info:\n",
    "                log_episode_info(writer, global_step, info)\n",
    "            \n",
    "            # Check if episode is done\n",
    "            if terminated or truncated:\n",
    "                print(f\"Episode Done {update} - resetting environment\")\n",
    "                next_obs, _ = env.reset()\n",
    "            \n",
    "            obs = next_obs\n",
    "        \n",
    "        # Update PPO agent\n",
    "        ppo_agent.update()\n",
    "        \n",
    "        # Log training metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"charts/learning_rate\": args.learning_rate,\n",
    "                \"train/mean_reward\": np.mean(ppo_agent.buffer.rewards),\n",
    "                \"train/episode_length\": len(ppo_agent.buffer.rewards)\n",
    "            }, step=global_step)\n",
    "        \n",
    "        # Save model periodically\n",
    "        if update % 10 == 0:\n",
    "            save_path = f\"models/{run_name}/checkpoint_{update}.pt\"\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            ppo_agent.save(save_path)\n",
    "            if args.track:\n",
    "                wandb.save(save_path)\n",
    "    \n",
    "    # Save final model\n",
    "    save_path = f\"models/{run_name}/model_final.pt\"\n",
    "    ppo_agent.save(save_path)\n",
    "    if args.track:\n",
    "        wandb.save(save_path)\n",
    "    \n",
    "    # env.close()\n",
    "    \n",
    "    writer.close()\n",
    "    return ppo_agent, env\n",
    "\n",
    "def evaluate_policy(env, ppo_agent, num_episodes=4):\n",
    "    \"\"\"Evaluate the trained policy\"\"\"\n",
    "    os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "    episode_rewards = np.zeros((num_episodes, env.sim_settings.n_points))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_errors = np.zeros((num_episodes, env.sim_settings.n_points))\n",
    "    episode_cpu_times = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get actions for all points\n",
    "            actions = []\n",
    "            for point_obs in obs:\n",
    "                action = ppo_agent.select_action(point_obs)\n",
    "                actions.append(action)\n",
    "            actions = np.array(actions)\n",
    "            \n",
    "            obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_rewards[episode] += rewards\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode Done {episode}\")\n",
    "                episode_lengths[episode] = episode_length\n",
    "                episode_errors[episode] = info['point_errors']\n",
    "                episode_cpu_times[episode] = info['cpu_time']\n",
    "                \n",
    "                print(f\"Cummulative Episode Reward: {np.sum(episode_rewards[episode])}\")\n",
    "                print(f\"Cummulative Episode Error: {np.sum(episode_errors[episode])}\")\n",
    "                print(f\"Cummulative Episode CPU Time: {np.sum(episode_cpu_times[episode])}\")\n",
    "    \n",
    "    # Compute and return statistics\n",
    "    results = {\n",
    "        'mean_reward_per_point': np.mean(episode_rewards, axis=0),\n",
    "        'std_reward_per_point': np.std(episode_rewards, axis=0),\n",
    "        'mean_error_per_point': np.mean(episode_errors, axis=0),\n",
    "        'mean_episode_length': np.mean(episode_lengths),\n",
    "        'mean_cpu_time': np.mean(episode_cpu_times),\n",
    "        'total_rewards': np.sum(episode_rewards),\n",
    "        'max_error': np.max(episode_errors)\n",
    "    }\n",
    "    \n",
    "    # Plot results\n",
    "    plot_evaluation_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_evaluation_results(results):\n",
    "    \"\"\"Plot evaluation results\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Reward distribution\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results['mean_reward_per_point'], label='Mean Reward')\n",
    "    plt.fill_between(\n",
    "        range(len(results['mean_reward_per_point'])),\n",
    "        results['mean_reward_per_point'] - results['std_reward_per_point'],\n",
    "        results['mean_reward_per_point'] + results['std_reward_per_point'],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.title('Reward Distribution Across Grid Points')\n",
    "    plt.xlabel('Grid Point')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(results['mean_error_per_point'], 'r-', label='Mean Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Error Distribution Across Grid Points')\n",
    "    plt.xlabel('Grid Point')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation/point_distributions.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(cuda=False)\n",
    "\n",
    "# Set default configurations if not provided\n",
    "if args.features_config is None:\n",
    "    args.features_config = {\n",
    "        'local_features': True,\n",
    "        'neighbor_features': False,\n",
    "        'gradient_features': False,\n",
    "        'temporal_features': False,\n",
    "        'window_size': 1\n",
    "    }\n",
    "\n",
    "if args.reward_config is None:\n",
    "    args.reward_config = {\n",
    "        'weights': {\n",
    "            'accuracy': 0.4,\n",
    "            'efficiency': 0.3,\n",
    "            'stability': 0.3\n",
    "        },\n",
    "        'thresholds': {\n",
    "            'time': 0.01,\n",
    "            'error': 1e-3\n",
    "        },\n",
    "        'scaling': {\n",
    "            'time': 0.1,\n",
    "            'error': 1.0\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = make_env(args)\n",
    "\n",
    "# PPO agent setup\n",
    "state_dim = env.observation_space.shape[1]\n",
    "action_dim = len(env.integrator_options)\n",
    "\n",
    "ppo_agent = PPO(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr_actor=args.learning_rate,\n",
    "    lr_critic=args.learning_rate,\n",
    "    gamma=args.gamma,\n",
    "    K_epochs=args.update_epochs,\n",
    "    eps_clip=args.eps_clip,\n",
    "    has_continuous_action_space=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = \"models/combustion_ppo_1d__1__1736895993/checkpoint_140.pt\"\n",
    "ppo_agent.load(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "obs, _ = env.reset(seed=args.seed)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "\n",
    "# Calculate number of updates\n",
    "total_timesteps = args.total_timesteps\n",
    "num_updates = total_timesteps // args.num_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actions = []\n",
    "for point_obs in obs:\n",
    "    action = ppo_agent.select_action(point_obs, deterministic=False)\n",
    "    actions.append(action)\n",
    "actions = np.array(actions)\n",
    "print(actions)\n",
    "# next_obs, rewards, terminated, truncated, info = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, ppo_agent, num_episodes=4):\n",
    "    \"\"\"Evaluate the trained policy\"\"\"\n",
    "    os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "    episode_rewards = np.zeros((num_episodes, env.sim_settings.n_points))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_errors = np.zeros((num_episodes, env.sim_settings.n_points))\n",
    "    episode_cpu_times = np.zeros(num_episodes)\n",
    "    \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_length = 0\n",
    "        episode_actions = []\n",
    "        while not done:\n",
    "            # Get actions for all points\n",
    "            actions = []\n",
    "            for point_obs in obs:\n",
    "                action = ppo_agent.select_action(point_obs, deterministic=True)\n",
    "                actions.append(action)\n",
    "            actions = np.array(actions)\n",
    "            episode_actions.append(actions)\n",
    "            print(f\"Number of 0 actions: {np.sum(actions == 0)} - Number of 1 actions: {np.sum(actions == 1)}\")\n",
    "            obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_rewards[episode] += rewards\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode Done {episode}\")\n",
    "                episode_lengths[episode] = episode_length\n",
    "                episode_errors[episode] = info['point_errors']\n",
    "                episode_cpu_times[episode] = info['cpu_time']\n",
    "                \n",
    "                print(f\"Cummulative Episode Reward: {np.sum(episode_rewards[episode])}\")\n",
    "                print(f\"Cummulative Episode Error: {np.sum(episode_errors[episode])}\")\n",
    "                print(f\"Cummulative Episode CPU Time: {np.sum(episode_cpu_times[episode])}\")\n",
    "    \n",
    "    # Compute and return statistics\n",
    "    results = {\n",
    "        'mean_reward_per_point': np.mean(episode_rewards, axis=0),\n",
    "        'std_reward_per_point': np.std(episode_rewards, axis=0),\n",
    "        'mean_error_per_point': np.mean(episode_errors, axis=0),\n",
    "        'mean_episode_length': np.mean(episode_lengths),\n",
    "        'mean_cpu_time': np.mean(episode_cpu_times),\n",
    "        'total_rewards': np.sum(episode_rewards),\n",
    "        'max_error': np.max(episode_errors)\n",
    "    }\n",
    "    \n",
    "    # Plot results\n",
    "    plot_evaluation_results(results)\n",
    "    \n",
    "    return results, episode_actions\n",
    "\n",
    "def plot_evaluation_results(results):\n",
    "    \"\"\"Plot evaluation results\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Reward distribution\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results['mean_reward_per_point'], label='Mean Reward')\n",
    "    plt.fill_between(\n",
    "        range(len(results['mean_reward_per_point'])),\n",
    "        results['mean_reward_per_point'] - results['std_reward_per_point'],\n",
    "        results['mean_reward_per_point'] + results['std_reward_per_point'],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.title('Reward Distribution Across Grid Points')\n",
    "    plt.xlabel('Grid Point')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(results['mean_error_per_point'], 'r-', label='Mean Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Error Distribution Across Grid Points')\n",
    "    plt.xlabel('Grid Point')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation/point_distributions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, action_list = evaluate_policy(env, ppo_agent, num_episodes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = results[1]\n",
    "\n",
    "\n",
    "points_to_compare = 0, 500, 1000, 1500, 2000, 3000\n",
    "\n",
    "fig, ax = plt.subplots(len(points_to_compare)//2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, point in enumerate(points_to_compare):\n",
    "    ax[i//2, i%2].plot(action_list[point], label='boostRK')\n",
    "    ax[i//2, i%2].set_title(f'Actions at point {point}')\n",
    "    ax[i//2, i%2].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
